import argparse
import sys
import json
import os

import pandas as pd
from datetime import datetime
from ImageCropGenerator import GenerateCropsFromFile
from FileManagement import imagesFromVideos, parseMD, filterImages, symlinkClassification
from DetectMD import load_and_run_detector_batch
from tensorflow import keras


def main():
    parser = argparse.ArgumentParser(description='Workflow for running animl')

    parser.add_argument(
        'image_dir',
        type=str,
        help='A directory of images and/or videos to be processed')

    parser.add_argument(
        'megadetector_model',
        type=str,
        help='Path to the megadetector model')

    parser.add_argument(
        'classification_model',
        type=str,
        help='Path to the classification model')

    parser.add_argument(
        'classes',
        type=str,
        help='Path to the classes text file for the classification model')

    parser.add_argument(
        '-output_dir',
        default=None,
        type=str,
        help='Output directory including site name and date')

    parser.add_argument(
        '--fps',
        type=int,
        default=None,
        help='Number of frames per second to be used when extracting images from videos')

    parser.add_argument(
        '--frames',
        type=int,
        default=5,
        help='Number of frames to be extracted from videos')

    parser.add_argument(
        '--parallel',
        action='store_true',
        help='Run the program in a parallel fashion using multiple cores')

    parser.add_argument(
        '--threshold',
        type=float,
        default=0.9,
        help="Confidence threshold between 0 and 1.0. Default is 0.9")

    parser.add_argument(
        '--checkpoint_frequency',
        type=int,
        default=2500,  # approximately every hour
        help='Write results to a temporary file every N images; a value of -1 disables this feature')

    parser.add_argument(
        '--resume_from_checkpoint',
        help='Initiate from the specified checkpoint, which is in the same directory as the output_file specified')

    if len(sys.argv[1:]) == 0:
        parser.print_help()
        parser.exit()

    args = parser.parse_args()
    MegaDetector_file = args.megadetector_model
    Classification_file = args.classification_model
    classes = args.classes
    # load the checkpoint if available
    # relative file names are only output at the end; all file paths in the checkpoint are still full paths
    if args.resume_from_checkpoint:
        assert os.path.exists(args.resume_from_checkpoint), 'File at resume_from_checkpoint specified does not exist'
        with open(args.resume_from_checkpoint) as f:
            saved = json.load(f)
        assert 'images' in saved, \
            'The file saved as checkpoint does not have the correct fields; cannot be restored'
        results = saved['images']
        print('Restored {} entries from the checkpoint'.format(len(results)))
    else:
        results = []
    images = imagesFromVideos(args.image_dir, out_dir=args.output_dir, fps=args.fps, frames=args.frames, parallel=args.parallel)

    # test that we can write to the output_file's dir if checkpointing requested
    if args.checkpoint_frequency != -1:
        checkpoint_path = os.path.join(args.output_dir,
                                       'checkpoint_{}.json'.format(datetime.utcnow().strftime("%Y%m%d%H%M%S")))
        with open(checkpoint_path, 'w') as f:
            json.dump({'images': []}, f)
        print('The checkpoint file will be written to {}'.format(checkpoint_path))
    else:
        checkpoint_path = None

    # run MegaDetector and format results
    detections = load_and_run_detector_batch(images, MegaDetector_file, checkpoint_path, args.threshold,
                                             args.checkpoint_frequency,
                                             results)

    df = parseMD(detections)
    # filter out all non animal detections
    animalDataframe, otherDataframe = filterImages(df)
    animalDataframe = animalDataframe.reset_index(drop=True)
    otherDataframe = otherDataframe.reset_index(drop=True)
    otherDataframe = otherDataframe[['file', 'category']]
    otherDataframe = otherDataframe.rename(columns={'category': 'class'})
    if not otherDataframe.empty:
        for idx in range(0, len(otherDataframe.index)):
            category = otherDataframe.at[idx, 'class']
            if category == 2:
                otherDataframe.at[idx, 'class'] = 12
            else:
                otherDataframe.at[idx, 'class'] = 8
    # Create generator for classification model
    generator = GenerateCropsFromFile(animalDataframe)

    # Create and predict model
    model = keras.models.load_model(Classification_file)
    predictions = model.predict(generator)

    # Format Classification results
    predictionsDataframe = pd.DataFrame(predictions)
    maxDataframe = predictionsDataframe.idxmax(axis=1).to_frame(name='class')
    maxDataframe.insert(0, 'file', animalDataframe.iloc[:, 0].to_numpy())
    maxDataframe = maxDataframe.append(otherDataframe, ignore_index=True)

    # Read Classification Txt file
    table = pd.read_table(classes, sep=" ", index_col=0)
    for i in range(0, len(maxDataframe.index)):
        maxDataframe.at[i, 'class'] = table['x'].values[int(maxDataframe.at[i, 'class'])]

    # Symlink
    symlinkClassification(maxDataframe, args.output_dir, classes)


if __name__ == '__main__':
    main()
